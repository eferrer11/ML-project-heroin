{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10218671,"sourceType":"datasetVersion","datasetId":6316750},{"sourceId":10218677,"sourceType":"datasetVersion","datasetId":6316755}],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T13:36:39.537349Z","iopub.execute_input":"2024-12-17T13:36:39.537759Z","iopub.status.idle":"2024-12-17T13:36:39.570579Z","shell.execute_reply.started":"2024-12-17T13:36:39.537725Z","shell.execute_reply":"2024-12-17T13:36:39.569221Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/dataset-train2/train.csv\n/kaggle/input/dataset-test2/test.csv\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import torch\nfrom torch import Tensor\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, RandomizedSearchCV\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.decomposition import PCA\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom scipy.stats import uniform, randint, loguniform\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-17T13:36:39.573070Z","iopub.execute_input":"2024-12-17T13:36:39.573536Z","iopub.status.idle":"2024-12-17T13:36:39.581721Z","shell.execute_reply.started":"2024-12-17T13:36:39.573486Z","shell.execute_reply":"2024-12-17T13:36:39.580276Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"data_train = pd.read_csv(\"/kaggle/input/dataset-train2/train.csv\")\ndata_test = pd.read_csv(\"/kaggle/input/dataset-test2/test.csv\")\n\nX_train = data_train.iloc[:, 6:]\nX_test = data_test.iloc[:, 5:]\n\n# Encodage One-Hot sur les colonnes catégorielles\nclassifiers_train = data_train.iloc[:, 1:4]\nenc = OneHotEncoder(sparse_output=False, handle_unknown='ignore') \ndata_train_oh = enc.fit_transform(classifiers_train)\n\n# Appliquer la même transformation sur data_test\nclassifiers_test = data_test.iloc[:, 1:4]\ndata_test_oh = enc.transform(classifiers_test)\n\n# Convertir les résultats en DataFrame\ndata_train_oh = pd.DataFrame(data_train_oh)\ndata_test_oh = pd.DataFrame(data_test_oh)\n\n# Concaténer One-Hot avec les autres caractéristiques\nX_new_train = pd.concat([data_train_oh, X_train], axis=1)\nX_new_test = pd.concat([data_test_oh, X_test], axis=1)\n\nX_new_train.columns = X_new_train.columns.astype(str)\nX_new_test.columns = X_new_test.columns.astype(str)\n\ny = data_train[\"PURITY\"] / 100\n\n# Diviser les données en train et validation\nseed = 43\nX_train, X_valid, y_train, y_valid = train_test_split(X_new_train, y, test_size=0.2, random_state=42)\n\n# Standardisation des données (avant PCA)\nscaler = StandardScaler()\nX_train_standardized = scaler.fit_transform(X_train)\nX_valid_standardized = scaler.transform(X_valid)\nX_test_standardized = scaler.transform(X_new_test)\n\n# PCA sur les données standardisées\npca = PCA(n_components=42)  # 95% variance expliquée\nX_train_pca = pca.fit_transform(X_train_standardized)\nX_valid_pca = pca.transform(X_valid_standardized)\nX_test_pca = pca.transform(X_test_standardized)\n\n# Convertir en tensors PyTorch\nX_train_tensor = torch.tensor(X_train_pca, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\nX_valid_tensor = torch.tensor(X_valid_pca, dtype=torch.float32)\ny_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32).reshape(-1, 1)\nX_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n\n# Vérifier les tailles\nprint(\"X_train_tensor size:\", X_train_tensor.size())\nprint(\"X_valid_tensor size:\", X_valid_tensor.size())\nprint(\"X_test_tensor size:\", X_test_tensor.size())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T13:36:39.583450Z","iopub.execute_input":"2024-12-17T13:36:39.583936Z","iopub.status.idle":"2024-12-17T13:36:39.901122Z","shell.execute_reply.started":"2024-12-17T13:36:39.583884Z","shell.execute_reply":"2024-12-17T13:36:39.895972Z"}},"outputs":[{"name":"stdout","text":"X_train_tensor size: torch.Size([1040, 16])\nX_valid_tensor size: torch.Size([260, 16])\nX_test_tensor size: torch.Size([608, 16])\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## Neuron class","metadata":{}},{"cell_type":"code","source":"# Définir le modèle de réseau de neurones simple\n\nclass MyLoss(nn.Module):  # Hérite de nn.Module\n    def __init__(self, reduction: str = 'mean'):\n        super(MyLoss, self).__init__()\n        self.reduction = reduction\n\n    def forward(self, output: Tensor, target: Tensor) -> Tensor:\n        # Calcul de la perte personnalisée\n        loss = torch.maximum(torch.zeros_like(output), (output - target) - 0.05) + \\\n               torch.maximum(torch.zeros_like(target), (target - output) - 0.05)\n\n        # Applique la réduction spécifiée\n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        else:  # 'none'\n            return loss\n\nclass FeedForwardNN(nn.Module):\n\n    def __init__(self, input_size, lin_layer_sizes,outpout_size, lin_layer_dropouts, activation):\n\n        super().__init__()\n\n        if activation == 0:\n            self.activation = nn.ReLU()\n        elif activation == 1:\n            self.activation = nn.SiLU()\n        elif activation == 2:\n            self.activation = nn.Tanh()\n        elif activation == 3:\n            self.activation = nn.LeakyReLU()\n\n        current_input_size = input_size\n\n        # Création des couches linéaires en fonction des tailles spécifiées\n        self.lin_layers = nn.ModuleList()\n        for layer_size in lin_layer_sizes:\n            self.lin_layers.append(nn.Linear(current_input_size, layer_size))\n            current_input_size = layer_size  # Mise à jour de current_input_size\n\n        # Couche de sortie : doit correspondre à la sortie de la dernière couche linéaire\n        self.outpout_layer = nn.Linear(lin_layer_sizes[-1], outpout_size)\n        \n        # Création des couches de Dropout\n        self.dropout_layers = nn.ModuleList([nn.Dropout(rate) for rate in lin_layer_dropouts])\n    \n    def forward(self, x):\n\n        for lin_layer, dropout_layer in zip(self.lin_layers, self.dropout_layers):\n            x = lin_layer(x)\n            x = self.activation(x)\n            x = dropout_layer(x)\n        x = self.outpout_layer(x)\n        x = nn.Sigmoid()(x)\n\n        return x\n\n# Définir la classe NeuralNetRegressor\n\nclass NeuralNetRegressor(BaseEstimator, RegressorMixin):\n    def __init__(self, input_size, random_state, eta=0.001, max_epochs=100, batch=10, lin_layer_sizes = [50, 50],\n                 outpout_size = 1, lin_layer_dropouts = [0.4, 0.4], activation = 0):\n\n        self.input_size = input_size\n        self.random_state = random_state\n        self.eta = eta\n        self.max_epochs = max_epochs\n        self.batch = batch\n        self.lin_layer_sizes = lin_layer_sizes\n        self.outpout_size = outpout_size\n        self.lin_layer_dropouts = lin_layer_dropouts\n        self.activation = activation\n        self.model = FeedForwardNN(input_size, lin_layer_sizes,outpout_size, lin_layer_dropouts, activation)\n        self.criterion = nn.L1Loss()\n\n    def fit(self, X, y, do_print=False):\n        #optimizer = optim.Adam(self.model.parameters(), lr=self.eta, weight_decay=1e-4)\n        optimizer = torch.optim.SGD(self.parameters(), lr=self.eta, momentum=0.9, weight_decay=1e-4)\n        X_tensor = torch.tensor(X).clone().detach().float()\n        y_tensor = torch.tensor(y).clone().detach().float()\n        dataset = TensorDataset(X_tensor, y_tensor)\n        dataloader = DataLoader(dataset, batch_size=self.batch, shuffle=True)\n        self.model.train()\n\n        # Training loop\n        for epoch in range(self.max_epochs):\n            epoch_loss = 0.0\n            for batch_X, batch_y in dataloader:\n                optimizer.zero_grad()  # Reset gradients\n                outputs = self.model(batch_X)  # Forward pass\n                loss = self.criterion(outputs, batch_y)  # Compute loss\n                loss.backward()  # Backward pass\n                optimizer.step()  # Update parameters\n                epoch_loss += loss.item()\n\n                del batch_X, batch_y\n            \n            if do_print:\n                print(f\"Epoch {epoch+1}/{self.max_epochs}, Loss: {epoch_loss / len(dataloader)}\")\n\n        return self\n\n    def predict(self, X):\n\n        self.model.eval()\n        X_tensor = torch.tensor(X, dtype=torch.float32)\n        with torch.no_grad():\n            outputs = self.model(X_tensor).flatten()\n\n        return outputs.numpy()    \n\n    def parameters(self):\n        return self.model.parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T13:36:39.905894Z","iopub.execute_input":"2024-12-17T13:36:39.909170Z","iopub.status.idle":"2024-12-17T13:36:39.983231Z","shell.execute_reply.started":"2024-12-17T13:36:39.909115Z","shell.execute_reply":"2024-12-17T13:36:39.980354Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## RandomSearch","metadata":{}},{"cell_type":"code","source":"# Initialiser le modèle\n\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ninput_size = X_train_tensor.shape[1]\nnet = NeuralNetRegressor(input_size=input_size, random_state=seed)\n\n# Définir les paramètres pour GridSearch\nparams_dist = {\n\n    'eta': loguniform(1e-4, 1e-1),\n    'max_epochs': randint(200, 500),\n    'batch': randint(32, 70),\n    'lin_layer_sizes': [[randint.rvs(32, 128) for _ in range(randint.rvs(1, 5))]],  \n    'lin_layer_dropouts': [[uniform.rvs(0, 0.5) for _ in range(randint.rvs(1, 5))]],\n    'activation': randint(0, 4),\n}\n\n# Initialiser RandomizedSearchCV\nrandom_search = RandomizedSearchCV(net, params_dist, refit=True, cv=5, random_state=seed, scoring='neg_mean_absolute_error', verbose=0, n_iter = 10)\n\n# Entraîner le modèle avec GridSearch\nrandom_grid_result = random_search.fit(X_train_tensor, y_train_tensor)\nnouveau_model = random_grid_result.best_estimator_\n\nprint(\"Best MSE: %f using %s\" % (random_grid_result.best_score_, random_grid_result.best_params_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T13:36:39.986128Z","iopub.execute_input":"2024-12-17T13:36:39.990025Z","iopub.status.idle":"2024-12-17T13:51:41.468850Z","shell.execute_reply.started":"2024-12-17T13:36:39.989963Z","shell.execute_reply":"2024-12-17T13:51:41.467677Z"}},"outputs":[{"name":"stdout","text":"Best MSE: -0.043757 using {'activation': 3, 'batch': 22, 'eta': 0.009960259174680096, 'lin_layer_dropouts': [0.05145737951022, 0.07438636288266487, 0.08606628077901546, 0.4055181959984358], 'lin_layer_sizes': [96], 'max_epochs': 491}\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"## Prédictions","metadata":{}},{"cell_type":"code","source":"y_pred = nouveau_model.predict(X_valid_tensor)\ny_pred_train = nouveau_model.predict(X_train_tensor)\npredictions = nouveau_model.predict(X_test_tensor)*100\n\n# Vérifiez les sorties du modèle\nprint(\"y_pred contains NaN:\", np.isnan(y_pred).any())\n# Calculer la MSE\nmse = np.mean(((y_pred - y_valid)*100)**2)\nprint(\"MSE :\", mse)\n\n# Calculer le t_score\ntrain_score = np.mean(np.abs(y_pred_train-y_train)*100<=5)\ntest_score = np.mean(np.abs(y_pred-y_valid)*100<=5)\nprint(\"t_score test :\", train_score)\nprint(\"t_score train :\", test_score)\n\nids = np.arange(1, len(predictions) + 1)\n\n# Create a DataFrame for the output\noutput_df = pd.DataFrame({\n\n    'ID': ids,\n\n    'PURITY': predictions\n\n})\n\n# Save the DataFrame to a CSV file\noutput_df.to_csv('predictions.csv', index=False) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T13:51:41.470834Z","iopub.execute_input":"2024-12-17T13:51:41.471315Z","iopub.status.idle":"2024-12-17T13:51:41.490326Z","shell.execute_reply.started":"2024-12-17T13:51:41.471266Z","shell.execute_reply":"2024-12-17T13:51:41.488848Z"}},"outputs":[{"name":"stdout","text":"y_pred contains NaN: False\nMSE : 38.15832206233432\nt_score test : 0.7528846153846154\nt_score train : 0.6923076923076923\n","output_type":"stream"}],"execution_count":31}]}