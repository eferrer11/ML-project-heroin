{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5df26b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T19:58:30.138195Z",
     "iopub.status.busy": "2024-12-16T19:58:30.137684Z",
     "iopub.status.idle": "2024-12-16T19:58:31.229867Z",
     "shell.execute_reply": "2024-12-16T19:58:31.228680Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 1.10023,
     "end_time": "2024-12-16T19:58:31.232270",
     "exception": false,
     "start_time": "2024-12-16T19:58:30.132040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/dataset-train2/train.csv\n",
      "/kaggle/input/dataset-test2/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91a87a03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T19:58:31.240693Z",
     "iopub.status.busy": "2024-12-16T19:58:31.240166Z",
     "iopub.status.idle": "2024-12-16T19:58:36.785304Z",
     "shell.execute_reply": "2024-12-16T19:58:36.784246Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 5.552027,
     "end_time": "2024-12-16T19:58:36.787876",
     "exception": false,
     "start_time": "2024-12-16T19:58:31.235849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.stats import uniform, randint, loguniform\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae07984",
   "metadata": {
    "papermill": {
     "duration": 0.002988,
     "end_time": "2024-12-16T19:58:36.794167",
     "exception": false,
     "start_time": "2024-12-16T19:58:36.791179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "569191df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T19:58:36.802763Z",
     "iopub.status.busy": "2024-12-16T19:58:36.801645Z",
     "iopub.status.idle": "2024-12-16T19:58:37.249930Z",
     "shell.execute_reply": "2024-12-16T19:58:37.248188Z"
    },
    "papermill": {
     "duration": 0.460841,
     "end_time": "2024-12-16T19:58:37.258096",
     "exception": false,
     "start_time": "2024-12-16T19:58:36.797255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor size: torch.Size([1040, 16])\n",
      "X_valid_tensor size: torch.Size([260, 16])\n",
      "X_test_tensor size: torch.Size([608, 16])\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"/kaggle/input/dataset-train2/train.csv\")\n",
    "data_test = pd.read_csv(\"/kaggle/input/dataset-test2/test.csv\")\n",
    "\n",
    "X_train = data_train.iloc[:, 6:]\n",
    "X_test = data_test.iloc[:, 5:]\n",
    "\n",
    "# Encodage One-Hot sur les colonnes catégorielles\n",
    "classifiers_train = data_train.iloc[:, 1:4]\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore') \n",
    "data_train_oh = enc.fit_transform(classifiers_train)\n",
    "\n",
    "# Appliquer la même transformation sur data_test\n",
    "classifiers_test = data_test.iloc[:, 1:4]\n",
    "data_test_oh = enc.transform(classifiers_test)\n",
    "\n",
    "# Convertir les résultats en DataFrame\n",
    "data_train_oh = pd.DataFrame(data_train_oh)\n",
    "data_test_oh = pd.DataFrame(data_test_oh)\n",
    "\n",
    "# Concaténer One-Hot avec les autres caractéristiques\n",
    "X_new_train = pd.concat([data_train_oh, X_train], axis=1)\n",
    "X_new_test = pd.concat([data_test_oh, X_test], axis=1)\n",
    "\n",
    "X_new_train.columns = X_new_train.columns.astype(str)\n",
    "X_new_test.columns = X_new_test.columns.astype(str)\n",
    "\n",
    "y = data_train[\"PURITY\"] / 100\n",
    "\n",
    "# Diviser les données en train et validation\n",
    "seed = 44\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_new_train, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Standardisation des données (avant PCA)\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_valid_standardized = scaler.transform(X_valid)\n",
    "X_test_standardized = scaler.transform(X_new_test)\n",
    "\n",
    "# PCA sur les données standardisées\n",
    "pca = PCA(n_components=16)  # 95% variance expliquée\n",
    "X_train_pca = pca.fit_transform(X_train_standardized)\n",
    "X_valid_pca = pca.transform(X_valid_standardized)\n",
    "X_test_pca = pca.transform(X_test_standardized)\n",
    "\n",
    "# Convertir en tensors PyTorch\n",
    "X_train_tensor = torch.tensor(X_train_pca, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
    "X_valid_tensor = torch.tensor(X_valid_pca, dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n",
    "\n",
    "# Vérifier les tailles\n",
    "print(\"X_train_tensor size:\", X_train_tensor.size())  # Devrait être (1040, 39)\n",
    "print(\"X_valid_tensor size:\", X_valid_tensor.size())  # Devrait être (260, 39)\n",
    "print(\"X_test_tensor size:\", X_test_tensor.size())  # Devrait être (608, 39)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a082791c",
   "metadata": {
    "papermill": {
     "duration": 0.013109,
     "end_time": "2024-12-16T19:58:37.286897",
     "exception": false,
     "start_time": "2024-12-16T19:58:37.273788",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Neuron class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31043f34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T19:58:37.301056Z",
     "iopub.status.busy": "2024-12-16T19:58:37.300654Z",
     "iopub.status.idle": "2024-12-16T19:58:37.319480Z",
     "shell.execute_reply": "2024-12-16T19:58:37.318401Z"
    },
    "papermill": {
     "duration": 0.029236,
     "end_time": "2024-12-16T19:58:37.321891",
     "exception": false,
     "start_time": "2024-12-16T19:58:37.292655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Définir le modèle de réseau de neurones simple\n",
    "\n",
    "class MyLoss(nn.Module):  # Hérite de nn.Module\n",
    "    def __init__(self, reduction: str = 'mean'):\n",
    "        super(MyLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, output: Tensor, target: Tensor) -> Tensor:\n",
    "        # Calcul de la perte personnalisée\n",
    "        loss = torch.maximum(torch.zeros_like(output), (output - target) - 0.05) + \\\n",
    "               torch.maximum(torch.zeros_like(target), (target - output) - 0.05)\n",
    "\n",
    "        # Applique la réduction spécifiée\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:  # 'none'\n",
    "            return loss\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, lin_layer_sizes,outpout_size, lin_layer_dropouts, activation):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if activation == 0:\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 1:\n",
    "            self.activation = nn.SiLU()\n",
    "        elif activation == 2:\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 3:\n",
    "            self.activation = nn.LeakyReLU()\n",
    "\n",
    "        # Linear Layers\n",
    "\n",
    "        first_lin_layer = nn.Linear(input_size, lin_layer_sizes[0])\n",
    "        self.lin_layers = nn.ModuleList([first_lin_layer] + [nn.Linear(lin_layer_sizes[i],\n",
    "                        lin_layer_sizes[i + 1]) for i in range(len(lin_layer_sizes) - 1)])\n",
    "    \n",
    "        # Output Layer\n",
    "        self.outpout_layer = nn.Linear(lin_layer_sizes[-1], outpout_size)\n",
    "    \n",
    "        # Dropout Layers\n",
    "        self.dropout_layers = nn.ModuleList([nn.Dropout(rate) for rate,size in zip(lin_layer_dropouts,lin_layer_sizes)])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for lin_layer, dropout_layer in zip(self.lin_layers, self.dropout_layers):\n",
    "            x = lin_layer(x)\n",
    "            x = self.activation(x)\n",
    "            x = dropout_layer(x)\n",
    "        x = self.outpout_layer(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Définir la classe NeuralNetRegressor\n",
    "\n",
    "class NeuralNetRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, input_size, random_state, eta=0.001, max_epochs=100, batch=10, lin_layer_sizes = [50, 50],\n",
    "                 outpout_size = 1, lin_layer_dropouts = [0.4, 0.4], activation = 0):\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.random_state = random_state\n",
    "        self.eta = eta\n",
    "        self.max_epochs = max_epochs\n",
    "        self.batch = batch\n",
    "        self.lin_layer_sizes = lin_layer_sizes\n",
    "        self.outpout_size = outpout_size\n",
    "        self.lin_layer_dropouts = lin_layer_dropouts\n",
    "        self.activation = activation\n",
    "        self.model = FeedForwardNN(input_size, lin_layer_sizes,outpout_size, lin_layer_dropouts, activation)\n",
    "        self.criterion = nn.L1Loss()\n",
    "\n",
    "    def fit(self, X, y, do_print=False):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.eta, weight_decay=1e-4)\n",
    "        X_tensor = torch.tensor(X).clone().detach().float()\n",
    "        y_tensor = torch.tensor(y).clone().detach().float()\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch, shuffle=True)\n",
    "        self.model.train()\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(self.max_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for batch_X, batch_y in dataloader:\n",
    "                optimizer.zero_grad()  # Reset gradients\n",
    "                outputs = self.model(batch_X)  # Forward pass\n",
    "                loss = self.criterion(outputs, batch_y)  # Compute loss\n",
    "                loss.backward()  # Backward pass\n",
    "                optimizer.step()  # Update parameters\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            if do_print:\n",
    "                print(f\"Epoch {epoch+1}/{self.max_epochs}, Loss: {epoch_loss / len(dataloader)}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        self.model.eval()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor).flatten()\n",
    "\n",
    "        return outputs.numpy()    \n",
    "\n",
    "    def parameters(self):\n",
    "        return self.model.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fefc70",
   "metadata": {
    "papermill": {
     "duration": 0.002835,
     "end_time": "2024-12-16T19:58:37.327844",
     "exception": false,
     "start_time": "2024-12-16T19:58:37.325009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d1a43cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T19:58:37.335379Z",
     "iopub.status.busy": "2024-12-16T19:58:37.334965Z",
     "iopub.status.idle": "2024-12-16T20:03:13.927808Z",
     "shell.execute_reply": "2024-12-16T20:03:13.926328Z"
    },
    "papermill": {
     "duration": 276.601452,
     "end_time": "2024-12-16T20:03:13.932238",
     "exception": false,
     "start_time": "2024-12-16T19:58:37.330786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MSE: -0.045261 using {'activation': 2, 'batch': 46, 'eta': 0.001518233791242678, 'lin_layer_dropouts': [0.01588740104675468, 0.3139252527076782], 'lin_layer_sizes': [67], 'max_epochs': 152}\n"
     ]
    }
   ],
   "source": [
    "# Initialiser le modèle\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "input_size = X_train_tensor.shape[1]\n",
    "net = NeuralNetRegressor(input_size=input_size, random_state=seed)\n",
    "\n",
    "# Définir les paramètres pour GridSearch\n",
    "params_dist = {\n",
    "\n",
    "    'eta': loguniform(1e-4, 1e-1),\n",
    "    'max_epochs': randint(130, 160),\n",
    "    'batch': randint(38, 50),\n",
    "    'lin_layer_sizes': [[randint.rvs(32, 128) for _ in range(randint.rvs(1, 4))]],  # Taille de 1 à 4 couches, entre 32 et 128 neurones par couche\n",
    "    'lin_layer_dropouts': [[uniform.rvs(0, 0.5) for _ in range(randint.rvs(1, 4))]],  # Dropout entre 0 et 0.5 pour chaque couche\n",
    "    'activation': randint(0, 4),\n",
    "}\n",
    "\n",
    "# Initialiser RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(net, params_dist, refit=True, cv=5, random_state=seed, scoring='neg_mean_absolute_error', verbose=0)\n",
    "\n",
    "# Entraîner le modèle avec GridSearch\n",
    "random_grid_result = random_search.fit(X_train_tensor, y_train_tensor)\n",
    "nouveau_model = random_grid_result.best_estimator_\n",
    "\n",
    "print(\"Best MSE: %f using %s\" % (random_grid_result.best_score_, random_grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c4b512",
   "metadata": {
    "papermill": {
     "duration": 0.002886,
     "end_time": "2024-12-16T20:03:13.939489",
     "exception": false,
     "start_time": "2024-12-16T20:03:13.936603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00727de9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T20:03:13.947757Z",
     "iopub.status.busy": "2024-12-16T20:03:13.947138Z",
     "iopub.status.idle": "2024-12-16T20:03:13.973699Z",
     "shell.execute_reply": "2024-12-16T20:03:13.972493Z"
    },
    "papermill": {
     "duration": 0.033423,
     "end_time": "2024-12-16T20:03:13.975965",
     "exception": false,
     "start_time": "2024-12-16T20:03:13.942542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred contains NaN: False\n",
      "t_score test : 0.8875\n",
      "t_score train : 0.8769230769230769\n"
     ]
    }
   ],
   "source": [
    "y_pred = nouveau_model.predict(X_valid_tensor)\n",
    "y_pred_train = nouveau_model.predict(X_train_tensor)\n",
    "predictions = nouveau_model.predict(X_test_tensor)*100\n",
    "\n",
    "# Vérifiez les sorties du modèle\n",
    "print(\"y_pred contains NaN:\", np.isnan(y_pred).any())\n",
    "# Calculer la MSE\n",
    "#mse = np.mean(((y_pred - y_valid)*100)**2)\n",
    "#print(\"MSE :\", mse)\n",
    "\n",
    "# Calculer le t_score\n",
    "train_score = np.mean(np.abs((y_pred_train-y_train)*100<=5))\n",
    "test_score = np.mean(np.abs((y_pred-y_valid)*100<=5))\n",
    "print(\"t_score test :\", train_score)\n",
    "print(\"t_score train :\", test_score)\n",
    "\n",
    "ids = np.arange(1, len(predictions) + 1)\n",
    "\n",
    "# Create a DataFrame for the output\n",
    "output_df = pd.DataFrame({\n",
    "\n",
    "    'ID': ids,\n",
    "\n",
    "    'PURITY': predictions\n",
    "\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_df.to_csv('predictions.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6316750,
     "sourceId": 10218671,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6316755,
     "sourceId": 10218677,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 288.21308,
   "end_time": "2024-12-16T20:03:15.304150",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-16T19:58:27.091070",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
