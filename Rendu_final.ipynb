{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import preprocessing\n",
    "from scipy.spatial.distance import cdist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, RandomizedSearchCV,train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.stats import uniform, randint, loguniform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the data\n",
    "data = pd.read_csv(\"./train.csv\")\n",
    "subspures = pd.read_csv(\"./substances.csv\")\n",
    "pure_heroin = subspures[(subspures['substance'] == 'heroin (white)') | (subspures[\"substance\"]==\"heroin (brown)\")]\n",
    "\n",
    "# Create new features\n",
    "distances = cdist(data.iloc[:,6:].to_numpy(), pure_heroin.iloc[:, 1:].to_numpy(), metric = 'euclidean')\n",
    "dist_her = pd.DataFrame(distances, index = data.iloc[:,6:].index, columns=pure_heroin.iloc[:,1:].index)\n",
    "data_new_features2 = data.iloc[:, 6:].values.dot(subspures.iloc[:,1:].values.T)\n",
    "\n",
    "\n",
    "X = data_new_features2\n",
    "y = data['PURITY']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "model = Ridge() #ridge regressors model\n",
    "param_grid = {'alpha': np.logspace(-7, 0, 100)} #hyperparamater alpha\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "\n",
    "mach1 = grid_search.best_estimator_ #best model\n",
    "mach1.fit(X_train, y_train) #apply the best model to the data\n",
    "predictions = mach1.predict(X_valid)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(np.logspace(-7, 0, 100),        \n",
    "            np.sqrt(-grid_search.cv_results_['mean_test_score']))\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y_pred1 = mach1.predict(X_train)\n",
    "y_pred2 = mach1.predict(X_valid)\n",
    "train_score = np.mean(np.abs(y_pred1-y_train<=5))\n",
    "test_score = np.mean(np.abs(y_pred2-y_valid<=5))\n",
    "print(\"Train score ridge :\", train_score)\n",
    "print(\"Test score ridge :\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-linear model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"./train.csv\")\n",
    "data_test = pd.read_csv(\"./test.csv\")\n",
    "X_train = data_train.iloc[:, 6:]\n",
    "X_test = data_test.iloc[:, 5:]\n",
    "\n",
    "y = data_train[\"PURITY\"]/100\n",
    "\n",
    "pca = PCA(n_components=16)\n",
    "X_pca = pd.DataFrame(pca.fit_transform(X_train))\n",
    "print(pca.explained_variance_ratio_) # to find the number of components to keep\n",
    "X_new = pd.concat([X_train, X_pca], axis=1)\n",
    "X_new.columns = X_new.columns.astype(str)\n",
    "standardizer = StandardScaler()\n",
    "X_st = standardizer.fit_transform(X_new)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_st, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
    "X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "X_pca = pd.DataFrame(pca.transform(X_test))\n",
    "X_new = pd.concat([X_test, X_pca], axis=1)\n",
    "X_new.columns = X_new.columns.astype(str)\n",
    "\n",
    "X_st = standardizer.transform(X_new)\n",
    "X_test_tensor = torch.tensor(X_st, dtype=torch.float32)\n",
    "\n",
    "# Define the FeedForwardNN class\n",
    "class FeedForwardNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, lin_layer_sizes,\n",
    "                 outpout_size, lin_layer_dropouts, activation):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        if activation == 0:\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 1:\n",
    "            self.activation = nn.SiLU()\n",
    "        elif activation == 2:\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 3:\n",
    "            self.activation = nn.LeakyReLU()\n",
    "\n",
    "        # Linear Layers\n",
    "        first_lin_layer = nn.Linear(input_size, lin_layer_sizes[0])\n",
    "    \n",
    "        self.lin_layers = nn.ModuleList([first_lin_layer] + [nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1]) for i in range(len(lin_layer_sizes) - 1)])\n",
    "      \n",
    "        # Output Layer\n",
    "        self.outpout_layer = nn.Linear(lin_layer_sizes[-1], outpout_size)\n",
    "    \n",
    "        # Dropout Layers\n",
    "        self.dropout_layers = nn.ModuleList([nn.Dropout(rate) for rate,size in zip(lin_layer_dropouts,lin_layer_sizes)])\n",
    "\n",
    "    def forward(self, x):\n",
    "  \n",
    "        for lin_layer, dropout_layer in zip(self.lin_layers, self.dropout_layers):\n",
    "\n",
    "            x = lin_layer(x)\n",
    "        \n",
    "            x = self.activation(x)\n",
    "\n",
    "            x = dropout_layer(x)\n",
    "      \n",
    "        x = self.outpout_layer(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "# Define the NeuralNetRegressor\n",
    "class NeuralNetRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, input_size, random_state, eta=0.001, max_epochs=100, batch=10, lin_layer_sizes = [50, 50],\n",
    "                 outpout_size = 1, lin_layer_dropouts = [0.4, 0.4], activation = 0):\n",
    "        self.input_size = input_size\n",
    "        self.random_state = random_state\n",
    "        self.eta = eta\n",
    "        self.max_epochs = max_epochs\n",
    "        self.batch = batch\n",
    "        self.lin_layer_sizes = lin_layer_sizes\n",
    "        self.outpout_size = outpout_size\n",
    "        self.lin_layer_dropouts = lin_layer_dropouts\n",
    "        self.activation = activation\n",
    "        self.model = FeedForwardNN(input_size, lin_layer_sizes,\n",
    "                 outpout_size, lin_layer_dropouts, activation)\n",
    "        self.criterion = nn.L1Loss()\n",
    "    \n",
    "    def fit(self, X, y, do_print=False):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.eta)\n",
    "        X_tensor = torch.tensor(X).clone().detach().float()\n",
    "        y_tensor = torch.tensor(y).clone().detach().float()\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch, shuffle=True)\n",
    "        self.model.train()\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(self.max_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for batch_X, batch_y in dataloader:\n",
    "                optimizer.zero_grad()  # Reset gradients\n",
    "                outputs = self.model(batch_X)  # Forward pass\n",
    "                loss = self.criterion(outputs, batch_y)  # Compute loss\n",
    "                loss.backward()  # Backward pass\n",
    "                optimizer.step()  # Update parameters\n",
    "                epoch_loss += loss.item()\n",
    "            if do_print:\n",
    "                print(f\"Epoch {epoch+1}/{self.max_epochs}, Loss: {epoch_loss / len(dataloader)}\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor).flatten()\n",
    "        return outputs.numpy()\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.model.parameters()\n",
    "    \n",
    "# Initialize the model\n",
    "seed = 43\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "input_size = X_train_tensor.shape[1]\n",
    "net = NeuralNetRegressor(input_size=input_size, random_state=43)\n",
    "\n",
    "# Define the parameters for GridSearch\n",
    "params_dist = {\n",
    "    'eta': loguniform(1e-4, 1e-1),\n",
    "    'max_epochs': randint(50, 150),\n",
    "    'batch': randint(32, 70),\n",
    "    'lin_layer_sizes': [[randint.rvs(32, 128) for _ in range(randint.rvs(1, 4))]],  # Taille de 1 Ã  4 couches, entre 32 et 128 neurones par couche\n",
    "    'lin_layer_dropouts': [[uniform.rvs(0, 0.5) for _ in range(randint.rvs(1, 4))]],  # Dropout entre 0 et 0.5 pour chaque couche\n",
    "    'activation': randint(0, 4),\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(net, params_dist, refit=True, cv=5, random_state=43, scoring='neg_mean_squared_error', verbose=0)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "random_grid_result = random_search.fit(X_train_tensor, y_train_tensor)\n",
    "nouveau_model = random_grid_result.best_estimator_\n",
    "\n",
    "print(\"Best MSE: %f using %s\" % (random_grid_result.best_score_, random_grid_result.best_params_))\n",
    "\n",
    "y_pred = nouveau_model.predict(X_valid_tensor)\n",
    "y_pred_train = nouveau_model.predict(X_train_tensor)\n",
    "predictions = nouveau_model.predict(X_test_tensor)*100\n",
    "\n",
    "# Verify that there are no NaN values in the predictions\n",
    "print(\"y_pred contains NaN:\", np.isnan(y_pred).any())\n",
    "\n",
    "# Calculater the loss\n",
    "mse = np.mean(((y_pred - y_valid)*100)**2)\n",
    "print(\"MSE :\", mse)\n",
    "\n",
    "# Calculate the t_score\n",
    "train_score = np.mean(np.abs((y_pred_train-y_train)*100<=5))\n",
    "test_score = np.mean(np.abs((y_pred-y_valid)*100<=5))\n",
    "print(\"t_score test :\", train_score)\n",
    "print(\"t_score train :\", test_score)\n",
    "\n",
    "ids = np.arange(1, len(predictions) + 1)\n",
    "\n",
    "# Create a DataFrame for the output\n",
    "output_df = pd.DataFrame({\n",
    "    'ID': ids,\n",
    "    'PURITY': predictions\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_df.to_csv('predictions.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
