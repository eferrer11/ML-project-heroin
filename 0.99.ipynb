{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10229122,"sourceType":"datasetVersion","datasetId":6324471},{"sourceId":10229125,"sourceType":"datasetVersion","datasetId":6324474}],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also writ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:22:37.504315Z","iopub.execute_input":"2024-12-17T18:22:37.504760Z","iopub.status.idle":"2024-12-17T18:22:37.517679Z","shell.execute_reply.started":"2024-12-17T18:22:37.504723Z","shell.execute_reply":"2024-12-17T18:22:37.516327Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/test-dataset2/test.csv\n/kaggle/input/train-dataset2/train.csv\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, RandomizedSearchCV\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.decomposition import PCA\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom scipy.stats import uniform, randint, loguniform\nfrom sklearn import preprocessing\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:09:20.150910Z","iopub.execute_input":"2024-12-18T16:09:20.152085Z","iopub.status.idle":"2024-12-18T16:09:20.159233Z","shell.execute_reply.started":"2024-12-18T16:09:20.152034Z","shell.execute_reply":"2024-12-18T16:09:20.157822Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"data_train = pd.read_csv(\"/kaggle/input/train-dataset2/train.csv\")\ndata_test = pd.read_csv(\"/kaggle/input/test-dataset2/test.csv\")\n\nX_train = data_train.iloc[:, 6:]\nX_test = data_test.iloc[:, 5:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:09:23.035408Z","iopub.execute_input":"2024-12-18T16:09:23.035897Z","iopub.status.idle":"2024-12-18T16:09:23.105241Z","shell.execute_reply.started":"2024-12-18T16:09:23.035854Z","shell.execute_reply":"2024-12-18T16:09:23.104091Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"y = data_train[\"PURITY\"]/100\nseed = 43\n\"\"\"\n# Combine original data with PCA components\nstandardizer = StandardScaler()\nX_st = standardizer.fit_transform(X_train)\n\"\"\"\n# Perform PCA to retain 95% of the variance\npca = PCA(42)\npca.fit(X_train)\nX_pca = pca.transform(X_train)\n\nX_normalised = preprocessing.normalize(X_pca)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_normalised, y, test_size=0.2, random_state = 42)\n\n# Convert to tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32).clone().detach()\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1).clone().detach()\nX_valid_tensor = torch.tensor(X_valid, dtype=torch.float32).clone().detach()\ny_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32).reshape(-1, 1).clone().detach()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:09:26.284698Z","iopub.execute_input":"2024-12-18T16:09:26.285133Z","iopub.status.idle":"2024-12-18T16:09:26.740096Z","shell.execute_reply.started":"2024-12-18T16:09:26.285094Z","shell.execute_reply":"2024-12-18T16:09:26.738862Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"X_pca = pca.transform(X_test)\nX_test_normalised = preprocessing.normalize(X_pca)\nX_test_tensor = torch.tensor(X_test_normalised, dtype=torch.float32).clone().detach() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:09:31.429240Z","iopub.execute_input":"2024-12-18T16:09:31.429856Z","iopub.status.idle":"2024-12-18T16:09:31.447658Z","shell.execute_reply.started":"2024-12-18T16:09:31.429794Z","shell.execute_reply":"2024-12-18T16:09:31.444833Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"y = data_train[\"PURITY\"] / 100\n\n# Diviser les données en train et validation\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y, test_size=0.2, random_state=42)\n\n# Standardisation des données (avant PCA)\nscaler = StandardScaler()\nX_train_standardized = scaler.fit_transform(X_train)\nX_valid_standardized = scaler.transform(X_valid)\nX_test_standardized = scaler.transform(X_test)\n\n# PCA sur les données standardisées\npca = PCA(n_components=42)  # 95% variance expliquée\nX_train_pca = pca.fit_transform(X_train_standardized)\nX_valid_pca = pca.transform(X_valid_standardized)\nX_test_pca = pca.transform(X_test_standardized)\n\n# Convertir en tensors PyTorch\nX_train_tensor = torch.tensor(X_train_pca, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\nX_valid_tensor = torch.tensor(X_valid_pca, dtype=torch.float32)\ny_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32).reshape(-1, 1)\nX_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:08:02.367494Z","iopub.execute_input":"2024-12-18T16:08:02.367899Z","iopub.status.idle":"2024-12-18T16:08:02.793928Z","shell.execute_reply.started":"2024-12-18T16:08:02.367861Z","shell.execute_reply":"2024-12-18T16:08:02.792089Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m y \u001b[38;5;241m=\u001b[39m data_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPURITY\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Diviser les données en train et validation\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m X_train, X_valid, y_train, y_valid \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Standardisation des données (avant PCA)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2559\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2559\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2561\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2562\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[1;32m   2564\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:443\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    442\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[0;32m--> 443\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1040, 1300]"],"ename":"ValueError","evalue":"Found input variables with inconsistent numbers of samples: [1040, 1300]","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"# Définir le modèle de réseau de neurones simple\n\nclass FeedForwardNN(nn.Module):\n\n    def __init__(self, input_size, lin_layer_sizes,\n\n                 outpout_size, lin_layer_dropouts, activation):\n        super().__init__()       \n        if activation == 0:\n            self.activation = nn.ReLU()\n        elif activation == 1:\n            self.activation = nn.SiLU()\n        elif activation == 2:\n            self.activation = nn.Tanh()\n        elif activation == 3:\n            self.activation = nn.LeakyReLU()  \n\n        # Linear Layers\n        first_lin_layer = nn.Linear(input_size, lin_layer_sizes[0])\n        self.lin_layers = nn.ModuleList([first_lin_layer] + [nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1]) for i in range(len(lin_layer_sizes) - 1)])     \n\n        # Output Layer\n        self.outpout_layer = nn.Linear(lin_layer_sizes[-1], outpout_size)\n        \n        # Dropout Layers\n        self.dropout_layers = nn.ModuleList([nn.Dropout(rate) for rate,size in zip(lin_layer_dropouts,lin_layer_sizes)])\n\n    def forward(self, x):\n        for lin_layer, dropout_layer in zip(self.lin_layers, self.dropout_layers):\n            x = lin_layer(x)     \n            x = self.activation(x)\n            x = dropout_layer(x)\n \n        x = self.outpout_layer(x)\n        x = nn.Sigmoid()(x)\n  \n        return x\n\n# Définir la classe NeuralNetRegressor\nclass NeuralNetRegressor(BaseEstimator, RegressorMixin):\n\n    def __init__(self, input_size, random_state, eta=0.001, max_epochs=100, batch=10, lin_layer_sizes = [50, 50],\n            outpout_size = 1, lin_layer_dropouts = [0.4, 0.4], activation = 0):\n\n        self.input_size = input_size\n        self.random_state = random_state\n        self.eta = eta\n        self.max_epochs = max_epochs\n        self.batch = batch\n        self.lin_layer_sizes = lin_layer_sizes\n        self.outpout_size = outpout_size\n        self.lin_layer_dropouts = lin_layer_dropouts\n        self.activation = activation\n        self.model = FeedForwardNN(input_size, lin_layer_sizes,\n                 outpout_size, lin_layer_dropouts, activation)\n\n        self.criterion = nn.MSELoss()\n    \n    def fit(self, X, y, do_print=False):\n\n        optimizer = optim.Adam(self.model.parameters(), lr=self.eta)\n        X_tensor = torch.tensor(X).clone().detach().float()\n        y_tensor = torch.tensor(y).clone().detach().float()\n        dataset = TensorDataset(X_tensor, y_tensor)\n        dataloader = DataLoader(dataset, batch_size=self.batch, shuffle=True)\n        self.model.train()\n\n        # Training loop\n        for epoch in range(self.max_epochs):\n\n            epoch_loss = 0.0\n            for batch_X, batch_y in dataloader:\n                optimizer.zero_grad()  # Reset gradients\n                outputs = self.model(batch_X)  # Forward pass\n                loss = self.criterion(outputs, batch_y)  # Compute loss\n                loss.backward()  # Backward pass\n                optimizer.step()  # Update parameters\n                epoch_loss += loss.item()\n            if do_print:\n\n                print(f\"Epoch {epoch+1}/{self.max_epochs}, Loss: {epoch_loss / len(dataloader)}\")\n\n        return self  \n\n    def predict(self, X):\n        self.model.eval()\n        X_tensor = torch.tensor(X, dtype=torch.float32)\n        with torch.no_grad():\n            outputs = self.model(X_tensor).flatten()\n        return outputs.numpy()\n        \n    def parameters(self):\n        return self.model.parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:12:26.917904Z","iopub.execute_input":"2024-12-18T17:12:26.918396Z","iopub.status.idle":"2024-12-18T17:12:26.939464Z","shell.execute_reply.started":"2024-12-18T17:12:26.918354Z","shell.execute_reply":"2024-12-18T17:12:26.938046Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"class RandomLayers:\n    def __init__(self,min_layers, max_layers, min_nodes, max_nodes):\n        self.min_nodes = min_nodes\n        self.max_nodes = max_nodes\n        self.min_layers = min_layers\n        self.max_layers = max_layers\n\n    def rvs(self, random_state=None):\n        if random_state is not None:\n            prev = randint.random_state\n            randint.random_state = random_state \n\n        res = [randint.rvs(self.min_nodes, self.max_nodes) for _ in range(randint.rvs(self.min_layers, self.max_layers))]\n\n        if random_state is not None:\n            randint.random_state = prev\n\n        return res","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:11:58.518750Z","iopub.execute_input":"2024-12-18T16:11:58.519266Z","iopub.status.idle":"2024-12-18T16:11:58.526839Z","shell.execute_reply.started":"2024-12-18T16:11:58.519225Z","shell.execute_reply":"2024-12-18T16:11:58.525704Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class RandomDropout:\n    def __init__(self, max_layers,min_dropout,max_dropout):\n        self.min_dropout = min_dropout\n        self.max_dropout = max_dropout\n        self.max_layers = max_layers\n\n    def rvs(self, random_state=None):\n        if random_state is not None:\n            prev = uniform.random_state\n            uniform.random_state = random_state \n        res = [uniform.rvs(self.min_dropout, self.max_dropout) for _ in range(self.max_layers)]\n\n        if random_state is not None:\n            uniform.random_state = prev\n\n        return res","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:12:01.048945Z","iopub.execute_input":"2024-12-18T16:12:01.049358Z","iopub.status.idle":"2024-12-18T16:12:01.056862Z","shell.execute_reply.started":"2024-12-18T16:12:01.049313Z","shell.execute_reply":"2024-12-18T16:12:01.055552Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Initialiser le modèle\n\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ninput_size = X_train_tensor.shape[1]\nnet = NeuralNetRegressor(input_size=input_size, random_state=seed)\n\nmax_layers = 6\n\n# Définir les paramètres pour GridSearch\nparams_dist = {\n\n    'eta': loguniform(1e-4, 1e-1),\n    'max_epochs': randint(200, 500),\n    'batch': randint(8, 32),\n    'lin_layer_sizes': RandomLayers(1,max_layers,32,256),  # Taille de 1 à 4 couches, entre 32 et 128 neurones par couche\n    'lin_layer_dropouts': RandomDropout(max_layers,0.,1.),\n    'activation': randint(0, 4),\n}\n\n# Initialiser RandomizedSearchCV\nrandom_search = RandomizedSearchCV(net, params_dist, refit=True, cv=5, random_state=seed, scoring='neg_mean_squared_error', verbose=3, n_iter=20)\n\n# Entraîner le modèle avec GridSearch\nrandom_grid_result = random_search.fit(X_train_tensor, y_train_tensor)\nnouveau_model = random_grid_result.best_estimator_\nprint(\"Best MSE: %f using %s\" % (random_grid_result.best_score_, random_grid_result.best_params_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:12:50.791447Z","iopub.execute_input":"2024-12-18T17:12:50.791927Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 20 candidates, totalling 100 fits\n[CV 1/5] END activation=2, batch=27, eta=0.07114476009343425, lin_layer_dropouts=[0.7319939418114051, 0.5986584841970366, 0.15601864044243652, 0.15599452033620265, 0.05808361216819946, 0.8661761457749352], lin_layer_sizes=[135, 183, 162, 181], max_epochs=457;, score=-0.007 total time=  24.9s\n[CV 2/5] END activation=2, batch=27, eta=0.07114476009343425, lin_layer_dropouts=[0.7319939418114051, 0.5986584841970366, 0.15601864044243652, 0.15599452033620265, 0.05808361216819946, 0.8661761457749352], lin_layer_sizes=[135, 183, 162, 181], max_epochs=457;, score=-0.009 total time=  24.8s\n[CV 3/5] END activation=2, batch=27, eta=0.07114476009343425, lin_layer_dropouts=[0.7319939418114051, 0.5986584841970366, 0.15601864044243652, 0.15599452033620265, 0.05808361216819946, 0.8661761457749352], lin_layer_sizes=[135, 183, 162, 181], max_epochs=457;, score=-0.005 total time=  25.1s\n[CV 4/5] END activation=2, batch=27, eta=0.07114476009343425, lin_layer_dropouts=[0.7319939418114051, 0.5986584841970366, 0.15601864044243652, 0.15599452033620265, 0.05808361216819946, 0.8661761457749352], lin_layer_sizes=[135, 183, 162, 181], max_epochs=457;, score=-0.005 total time=  24.8s\n[CV 5/5] END activation=2, batch=27, eta=0.07114476009343425, lin_layer_dropouts=[0.7319939418114051, 0.5986584841970366, 0.15601864044243652, 0.15599452033620265, 0.05808361216819946, 0.8661761457749352], lin_layer_sizes=[135, 183, 162, 181], max_epochs=457;, score=-0.007 total time=  24.6s\n[CV 1/5] END activation=3, batch=19, eta=0.06541210527692734, lin_layer_dropouts=[0.0007787658410143283, 0.9922115592912175, 0.6174815096277165, 0.6116531604882809, 0.007066305219717406, 0.023062425041415757], lin_layer_sizes=[90, 201, 251], max_epochs=387;, score=-0.005 total time=  27.7s\n[CV 2/5] END activation=3, batch=19, eta=0.06541210527692734, lin_layer_dropouts=[0.0007787658410143283, 0.9922115592912175, 0.6174815096277165, 0.6116531604882809, 0.007066305219717406, 0.023062425041415757], lin_layer_sizes=[90, 201, 251], max_epochs=387;, score=-0.007 total time=  28.0s\n[CV 3/5] END activation=3, batch=19, eta=0.06541210527692734, lin_layer_dropouts=[0.0007787658410143283, 0.9922115592912175, 0.6174815096277165, 0.6116531604882809, 0.007066305219717406, 0.023062425041415757], lin_layer_sizes=[90, 201, 251], max_epochs=387;, score=-0.006 total time=  28.3s\n[CV 4/5] END activation=3, batch=19, eta=0.06541210527692734, lin_layer_dropouts=[0.0007787658410143283, 0.9922115592912175, 0.6174815096277165, 0.6116531604882809, 0.007066305219717406, 0.023062425041415757], lin_layer_sizes=[90, 201, 251], max_epochs=387;, score=-0.005 total time=  28.0s\n[CV 5/5] END activation=3, batch=19, eta=0.06541210527692734, lin_layer_dropouts=[0.0007787658410143283, 0.9922115592912175, 0.6174815096277165, 0.6116531604882809, 0.007066305219717406, 0.023062425041415757], lin_layer_sizes=[90, 201, 251], max_epochs=387;, score=-0.012 total time=  28.1s\n[CV 1/5] END activation=3, batch=22, eta=0.0023345864076016252, lin_layer_dropouts=[0.7851759613930136, 0.19967378215835974, 0.5142344384136116, 0.5924145688620425, 0.046450412719997725, 0.6075448519014384], lin_layer_sizes=[104, 198, 49, 163, 120], max_epochs=213;, score=-0.001 total time=  12.9s\n[CV 2/5] END activation=3, batch=22, eta=0.0023345864076016252, lin_layer_dropouts=[0.7851759613930136, 0.19967378215835974, 0.5142344384136116, 0.5924145688620425, 0.046450412719997725, 0.6075448519014384], lin_layer_sizes=[104, 198, 49, 163, 120], max_epochs=213;, score=-0.001 total time=  13.0s\n[CV 3/5] END activation=3, batch=22, eta=0.0023345864076016252, lin_layer_dropouts=[0.7851759613930136, 0.19967378215835974, 0.5142344384136116, 0.5924145688620425, 0.046450412719997725, 0.6075448519014384], lin_layer_sizes=[104, 198, 49, 163, 120], max_epochs=213;, score=-0.001 total time=  12.8s\n[CV 4/5] END activation=3, batch=22, eta=0.0023345864076016252, lin_layer_dropouts=[0.7851759613930136, 0.19967378215835974, 0.5142344384136116, 0.5924145688620425, 0.046450412719997725, 0.6075448519014384], lin_layer_sizes=[104, 198, 49, 163, 120], max_epochs=213;, score=-0.001 total time=  13.2s\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"y_pred = nouveau_model.predict(X_valid_tensor)\ny_pred_train = nouveau_model.predict(X_train_tensor)\npredictions = nouveau_model.predict(X_test_tensor)*100\n\n# Vérifiez les sorties du modèle\nprint(\"y_pred contains NaN:\", np.isnan(y_pred).any())\n# Calculer la MSE\nmse = np.mean(((y_pred - y_valid)*100)**2)\nprint(\"MSE :\", mse)\n\n# Calculer le t_score\ntrain_score = np.mean(np.abs(y_pred_train-y_train)*100<=5)\ntest_score = np.mean(np.abs(y_pred-y_valid)*100<=5)\nprint(\"t_score test :\", train_score)\nprint(\"t_score train :\", test_score)\n\nids = np.arange(1, len(predictions) + 1)\n\n# Create a DataFrame for the output\noutput_df = pd.DataFrame({\n\n    'ID': ids,\n\n    'PURITY': predictions\n\n})\n\n# Save the DataFrame to a CSV file\noutput_df.to_csv('predictions.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:10:15.282542Z","iopub.execute_input":"2024-12-18T17:10:15.282963Z","iopub.status.idle":"2024-12-18T17:10:15.307765Z","shell.execute_reply.started":"2024-12-18T17:10:15.282927Z","shell.execute_reply":"2024-12-18T17:10:15.306565Z"}},"outputs":[{"name":"stdout","text":"y_pred contains NaN: False\nMSE : 17.89531156494539\nt_score test : 0.9259615384615385\nt_score train : 0.85\n","output_type":"stream"}],"execution_count":24}]}