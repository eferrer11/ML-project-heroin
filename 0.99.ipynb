{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10228270,"sourceType":"datasetVersion","datasetId":6323822},{"sourceId":10228277,"sourceType":"datasetVersion","datasetId":6323828}],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also writ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T16:48:05.007686Z","iopub.execute_input":"2024-12-17T16:48:05.008164Z","iopub.status.idle":"2024-12-17T16:48:05.025744Z","shell.execute_reply.started":"2024-12-17T16:48:05.008124Z","shell.execute_reply":"2024-12-17T16:48:05.024467Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/dataset-test3/test.csv\n/kaggle/input/data-set3/train.csv\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, RandomizedSearchCV\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.decomposition import PCA\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom scipy.stats import uniform, randint, loguniform\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T16:48:05.027834Z","iopub.execute_input":"2024-12-17T16:48:05.028505Z","iopub.status.idle":"2024-12-17T16:48:05.036785Z","shell.execute_reply.started":"2024-12-17T16:48:05.028450Z","shell.execute_reply":"2024-12-17T16:48:05.035495Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"data_train = pd.read_csv(\"/kaggle/input/data-set3/train.csv\")\ndata_test = pd.read_csv(\"/kaggle/input/dataset-test3/test.csv\")\n\nX_train = data_train.iloc[:, 6:]\nX_test = data_test.iloc[:, 5:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T16:48:05.038481Z","iopub.execute_input":"2024-12-17T16:48:05.039177Z","iopub.status.idle":"2024-12-17T16:48:05.122030Z","shell.execute_reply.started":"2024-12-17T16:48:05.039136Z","shell.execute_reply":"2024-12-17T16:48:05.120859Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"y = data_train[\"PURITY\"] / 100\n\n# Diviser les données en train et validation\nseed = 43\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y, test_size=0.2, random_state=42)\n\n# Standardisation des données (avant PCA)\nscaler = StandardScaler()\nX_train_standardized = scaler.fit_transform(X_train)\nX_valid_standardized = scaler.transform(X_valid)\nX_test_standardized = scaler.transform(X_test)\n\n# PCA sur les données standardisées\npca = PCA(n_components=42)  # 95% variance expliquée\nX_train_pca = pca.fit_transform(X_train_standardized)\nX_valid_pca = pca.transform(X_valid_standardized)\nX_test_pca = pca.transform(X_test_standardized)\n\n# Convertir en tensors PyTorch\nX_train_tensor = torch.tensor(X_train_pca, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\nX_valid_tensor = torch.tensor(X_valid_pca, dtype=torch.float32)\ny_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32).reshape(-1, 1)\nX_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T16:48:05.124330Z","iopub.execute_input":"2024-12-17T16:48:05.124720Z","iopub.status.idle":"2024-12-17T16:48:05.408092Z","shell.execute_reply.started":"2024-12-17T16:48:05.124683Z","shell.execute_reply":"2024-12-17T16:48:05.405456Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Définir le modèle de réseau de neurones simple\n\nclass FeedForwardNN(nn.Module):\n\n    def __init__(self, input_size, lin_layer_sizes,\n\n                 outpout_size, lin_layer_dropouts, activation):\n        super().__init__()       \n        if activation == 0:\n            self.activation = nn.ReLU()\n        elif activation == 1:\n            self.activation = nn.SiLU()\n        elif activation == 2:\n            self.activation = nn.Tanh()\n        elif activation == 3:\n            self.activation = nn.LeakyReLU()  \n\n        # Linear Layers\n        first_lin_layer = nn.Linear(input_size, lin_layer_sizes[0])\n        self.lin_layers = nn.ModuleList([first_lin_layer] + [nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1]) for i in range(len(lin_layer_sizes) - 1)])     \n\n        # Output Layer\n        self.outpout_layer = nn.Linear(lin_layer_sizes[-1], outpout_size)\n        \n        # Dropout Layers\n        self.dropout_layers = nn.ModuleList([nn.Dropout(rate) for rate,size in zip(lin_layer_dropouts,lin_layer_sizes)])\n\n    def forward(self, x):\n        for lin_layer, dropout_layer in zip(self.lin_layers, self.dropout_layers):\n            x = lin_layer(x)     \n            x = self.activation(x)\n            x = dropout_layer(x)\n \n        x = self.outpout_layer(x)\n        x = nn.Sigmoid()(x)\n  \n\n        return x\n\n# Définir la classe NeuralNetRegressor\nclass NeuralNetRegressor(BaseEstimator, RegressorMixin):\n\n    def __init__(self, input_size, random_state, eta=0.001, max_epochs=100, batch=10, lin_layer_sizes = [50, 50],\n            outpout_size = 1, lin_layer_dropouts = [0.4, 0.4], activation = 0):\n\n        self.input_size = input_size\n        self.random_state = random_state\n        self.eta = eta\n        self.max_epochs = max_epochs\n        self.batch = batch\n        self.lin_layer_sizes = lin_layer_sizes\n        self.outpout_size = outpout_size\n        self.lin_layer_dropouts = lin_layer_dropouts\n        self.activation = activation\n        self.model = FeedForwardNN(input_size, lin_layer_sizes,\n                 outpout_size, lin_layer_dropouts, activation)\n\n        self.criterion = nn.MSELoss()\n    \n    def fit(self, X, y, do_print=False):\n\n        optimizer = optim.Adam(self.model.parameters(), lr=self.eta)\n        X_tensor = torch.tensor(X).clone().detach().float()\n        y_tensor = torch.tensor(y).clone().detach().float()\n        dataset = TensorDataset(X_tensor, y_tensor)\n        dataloader = DataLoader(dataset, batch_size=self.batch, shuffle=True)\n        self.model.train()\n\n        # Training loop\n        for epoch in range(self.max_epochs):\n\n            epoch_loss = 0.0\n            for batch_X, batch_y in dataloader:\n                optimizer.zero_grad()  # Reset gradients\n                outputs = self.model(batch_X)  # Forward pass\n                loss = self.criterion(outputs, batch_y)  # Compute loss\n                loss.backward()  # Backward pass\n                optimizer.step()  # Update parameters\n                epoch_loss += loss.item()\n            if do_print:\n\n                print(f\"Epoch {epoch+1}/{self.max_epochs}, Loss: {epoch_loss / len(dataloader)}\")\n\n        return self  \n\n    def predict(self, X):\n        self.model.eval()\n        X_tensor = torch.tensor(X, dtype=torch.float32)\n        with torch.no_grad():\n            outputs = self.model(X_tensor).flatten()\n        return outputs.numpy()\n    def parameters(self):\n\n        return self.model.parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T17:18:46.721857Z","iopub.execute_input":"2024-12-17T17:18:46.722302Z","iopub.status.idle":"2024-12-17T17:18:46.739954Z","shell.execute_reply.started":"2024-12-17T17:18:46.722264Z","shell.execute_reply":"2024-12-17T17:18:46.738651Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"class RandomLayers:\n    def __init__(self,min_layers, max_layers, min_nodes, max_nodes):\n        self.min_nodes = min_nodes\n        self.max_nodes = max_nodes\n        self.min_layers = min_layers\n        self.max_layers = max_layers\n\n    def rvs(self, random_state=None):\n        if random_state is not None:\n            prev = randint.random_state\n            randint.random_state = random_state \n\n        res = [[randint.rvs(self.min_nodes, self.max_nodes) for _ in range(randint.rvs(self.min_layers, self.max_layers))]]\n\n        if random_state is not None:\n            randint.random_state = prev\n\n        return res","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T16:48:05.499717Z","iopub.execute_input":"2024-12-17T16:48:05.500441Z","iopub.status.idle":"2024-12-17T16:48:05.524801Z","shell.execute_reply.started":"2024-12-17T16:48:05.500356Z","shell.execute_reply":"2024-12-17T16:48:05.523614Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"class RandomDropout:\n    def __init__(self, max_layers,min_dropout,max_dropout):\n        self.min_dropout = min_dropout\n        self.max_dropout = max_dropout\n        self.max_layers = max_layers\n\n    def rvs(self, random_state=None):\n        if random_state is not None:\n            prev = uniform.random_state\n            uniform.random_state = random_state \n        res = [[uniform.rvs(self.min_dropout, self.max_dropout) for _ in range(self.max_layers)]]\n\n        if random_state is not None:\n            uniform.random_state = prev\n\n        return res","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T16:48:05.526357Z","iopub.execute_input":"2024-12-17T16:48:05.527208Z","iopub.status.idle":"2024-12-17T16:48:05.537142Z","shell.execute_reply.started":"2024-12-17T16:48:05.527167Z","shell.execute_reply":"2024-12-17T16:48:05.535722Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Initialiser le modèle\n\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ninput_size = X_train_tensor.shape[1]\nnet = NeuralNetRegressor(input_size=input_size, random_state=42)\n\nmax_layers = 4\n\n# Définir les paramètres pour GridSearch\nparams_dist = {\n\n    'eta': loguniform(1e-4, 1e-1),\n    'max_epochs': randint(200, 500),\n    'batch': randint(32, 70),\n    'lin_layer_sizes': [[randint.rvs(32, 128) for _ in range(randint.rvs(1, 4))]],  # Taille de 1 à 4 couches, entre 32 et 128 neurones par couche\n    'lin_layer_dropouts': [[uniform.rvs(0, 0.5) for _ in range(randint.rvs(1, 4))]],\n    'activation': randint(0, 4),\n}\n\n# Initialiser RandomizedSearchCV\nrandom_search = RandomizedSearchCV(net, params_dist, refit=True, cv=5, random_state=42, scoring='neg_mean_squared_error', verbose=0)\n\n# Entraîner le modèle avec GridSearch\nrandom_grid_result = random_search.fit(X_train_tensor, y_train_tensor)\nnouveau_model = random_grid_result.best_estimator_\nprint(\"Best MSE: %f using %s\" % (random_grid_result.best_score_, random_grid_result.best_params_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T17:18:53.290550Z","iopub.execute_input":"2024-12-17T17:18:53.290967Z","iopub.status.idle":"2024-12-17T17:29:48.534563Z","shell.execute_reply.started":"2024-12-17T17:18:53.290930Z","shell.execute_reply":"2024-12-17T17:29:48.533013Z"}},"outputs":[{"name":"stdout","text":"Best MSE: -0.000692 using {'activation': 0, 'batch': 56, 'eta': 0.0007476312062252305, 'lin_layer_dropouts': [0.38984550013638464, 0.2984250789732435, 0.22291637642679557], 'lin_layer_sizes': [83, 124, 46], 'max_epochs': 258}\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"y_pred = nouveau_model.predict(X_valid_tensor)\ny_pred_train = nouveau_model.predict(X_train_tensor)\npredictions = nouveau_model.predict(X_test_tensor)*100\n\n# Vérifiez les sorties du modèle\nprint(\"y_pred contains NaN:\", np.isnan(y_pred).any())\n# Calculer la MSE\nmse = np.mean(((y_pred - y_valid)*100)**2)\nprint(\"MSE :\", mse)\n\n# Calculer le t_score\ntrain_score = np.mean(np.abs(y_pred_train-y_train)*100<=5)\ntest_score = np.mean(np.abs(y_pred-y_valid)*100<=5)\nprint(\"t_score test :\", train_score)\nprint(\"t_score train :\", test_score)\n\nids = np.arange(1, len(predictions) + 1)\n\n# Create a DataFrame for the output\noutput_df = pd.DataFrame({\n\n    'ID': ids,\n\n    'PURITY': predictions\n\n})\n\n# Save the DataFrame to a CSV file\noutput_df.to_csv('predictions.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T17:31:57.872869Z","iopub.execute_input":"2024-12-17T17:31:57.873304Z","iopub.status.idle":"2024-12-17T17:31:57.895133Z","shell.execute_reply.started":"2024-12-17T17:31:57.873265Z","shell.execute_reply":"2024-12-17T17:31:57.893935Z"}},"outputs":[{"name":"stdout","text":"y_pred contains NaN: False\nMSE : 6.738044427463776\nt_score test : 0.975\nt_score train : 0.9346153846153846\n","output_type":"stream"}],"execution_count":40}]}