{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we observed the data to better understand the features i.e. observe the correlation they had between each other, the \"importance\" they had to predict the purity correctly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the preprocessing of both linear and non linear model we removed the first 6 columns of the dataset which were text.\n",
    "\n",
    "- We try to create new features from the pure heroin furnished in substances.csv by making a dot product between the original data and the pure heroin. It gave us nine new features that we use in the linear model.\n",
    "\n",
    "- For the non-linear model, we chose neural network thus we perform a standardization and a PCA to reduce the number of features used, and found out that only 16 features were necessary to explained the variance. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(data.iloc[:, 6:])\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "n_components = np.argmax(explained_variance >= 0.95) + 1\n",
    "print(f\"Number of principal components needed to explain 95% of the variance: {n_components}\")\n",
    "\n",
    "plt.plot(np.arange(1, len(explained_variance) + 1), explained_variance, marker='o')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Explained Variance')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA - Cumulative Explained Variance')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ridge regression we perform a gridSearch to find the best hyperparameters a notably the lambda parameter that we plot to find the interval of searching and be as precise as we could."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mach1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(np.logspace(-7, 0, 100), np.sqrt(-grid_search.cv_results_['mean_test_score']))\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
