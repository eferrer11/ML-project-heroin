{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.stats import uniform, randint, loguniform\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata = pd.concat([data_train, data_test], axis=0)\\nX = data.iloc[:, 6:]\\nclassifiers = data.iloc[:, 1:4]\\nenc = OneHotEncoder()\\ndata_oh = enc.fit_transform(classifiers).toarray()\\ndata_oh = pd.DataFrame(data_oh)\\n\\n# Nombre de lignes dans data_train\\nn_train = len(data_train)\\n\\n# Séparer data en data_train et data_test\\ndata_train_oh = data_oh.iloc[:n_train, :].reset_index(drop=True)\\ndata_test_oh = data_oh.iloc[n_train:, :].reset_index(drop=True)\\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"./train.csv\")\n",
    "data_test = pd.read_csv(\"./test.csv\")\n",
    "X_train = data_train.iloc[:, 6:]\n",
    "X_test = data_test.iloc[:, 5:]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "data = pd.concat([data_train, data_test], axis=0)\n",
    "X = data.iloc[:, 6:]\n",
    "classifiers = data.iloc[:, 1:4]\n",
    "enc = OneHotEncoder()\n",
    "data_oh = enc.fit_transform(classifiers).toarray()\n",
    "data_oh = pd.DataFrame(data_oh)\n",
    "\n",
    "# Nombre de lignes dans data_train\n",
    "n_train = len(data_train)\n",
    "\n",
    "# Séparer data en data_train et data_test\n",
    "data_train_oh = data_oh.iloc[:n_train, :].reset_index(drop=True)\n",
    "data_test_oh = data_oh.iloc[n_train:, :].reset_index(drop=True)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        4.275457\n",
       "1        6.491709\n",
       "2        5.203686\n",
       "3       11.019652\n",
       "4       10.888498\n",
       "          ...    \n",
       "1295    10.896534\n",
       "1296     0.714299\n",
       "1297    10.633639\n",
       "1298     7.150106\n",
       "1299     5.362656\n",
       "Length: 1300, dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.sum(X_train, axis=1)\n",
    "X_normalised = pd.DataFrame(preprocessing.normalize(X_train))\n",
    "np.sum(X_normalised, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       -47.242542\n",
       "1       -25.612119\n",
       "2       -34.833110\n",
       "3       410.474762\n",
       "4       321.051996\n",
       "           ...    \n",
       "1295     64.044146\n",
       "1296    -54.127140\n",
       "1297     22.617288\n",
       "1298    -20.594557\n",
       "1299    -33.917856\n",
       "Length: 1300, dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardizer = StandardScaler()\n",
    "X_st = pd.DataFrame(standardizer.fit_transform(X_train))\n",
    "np.sum(X_st, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spectrum_filtered = pd.DataFrame(savgol_filter(spectrum, 7, 3, deriv = 2, axis = 0))\n",
    "#spectrum_filtered_st = zscore(spectrum_filtered, axis = 1)\n",
    "\n",
    "y = data_train[\"PURITY\"]/100\n",
    "seed = 43\n",
    "\"\"\"\n",
    "# Combine original data with PCA components\n",
    "standardizer = StandardScaler()\n",
    "X_st = standardizer.fit_transform(X_train)\n",
    "\"\"\"\n",
    "# Perform PCA to retain 95% of the variance\n",
    "pca = PCA(42)\n",
    "pca.fit(X_train)\n",
    "X_pca = pca.transform(X_train)\n",
    "\n",
    "X_normalised = preprocessing.normalize(X_pca)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_normalised, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).clone().detach()\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1).clone().detach()\n",
    "X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32).clone().detach()\n",
    "y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32).reshape(-1, 1).clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_st = standardizer.transform(X_test)\n",
    "X_pca = pca.transform(X_st)\n",
    "X_test_normalised = preprocessing.normalize(X_test.values)\n",
    "X_test_tensor = torch.tensor(X_pca, dtype=torch.float32).clone().detach() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "class MyLoss(nn.Module):  # Hérite de nn.Module\n",
    "    def __init__(self, reduction: str = 'mean'):\n",
    "        super(MyLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, output: Tensor, target: Tensor) -> Tensor:\n",
    "        # Calcul de la perte personnalisée\n",
    "        loss = torch.maximum(torch.zeros_like(output), (output - target) - 0.05) + \\\n",
    "               torch.maximum(torch.zeros_like(target), (target - output) - 0.05)\n",
    "\n",
    "        # Applique la réduction spécifiée\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:  # 'none'\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, threshold=5.0):\n",
    "        \"\"\"\n",
    "        Custom loss function to penalize predictions with an absolute percentage error greater than a threshold.\n",
    "        \n",
    "        Args:\n",
    "            threshold (float): The maximum percentage error allowed (in %).\n",
    "        \"\"\"\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute the custom loss.\n",
    "\n",
    "        Args:\n",
    "            y_pred (torch.Tensor): Predicted values.\n",
    "            y_true (torch.Tensor): Ground truth values.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The computed loss.\n",
    "        \"\"\"\n",
    "        # Compute absolute percentage error\n",
    "        percentage_error = torch.abs((y_pred - y_true) / y_true) * 100\n",
    "\n",
    "        # Mask for errors greater than the threshold\n",
    "        mask = (percentage_error > self.threshold).float()\n",
    "\n",
    "        # Penalize errors that exceed the threshold\n",
    "        loss = mask * percentage_error\n",
    "\n",
    "        # Return the mean loss\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir le modèle de réseau de neurones simple\n",
    "class FeedForwardNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, lin_layer_sizes,\n",
    "                 outpout_size, lin_layer_dropouts, activation):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        if activation == 0:\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 1:\n",
    "            self.activation = nn.SiLU()\n",
    "        elif activation == 2:\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 3:\n",
    "            self.activation = nn.LeakyReLU()\n",
    "\n",
    "    \n",
    "        # Linear Layers\n",
    "        first_lin_layer = nn.Linear(input_size, lin_layer_sizes[0])\n",
    "    \n",
    "        self.lin_layers = nn.ModuleList([first_lin_layer] + [nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1]) for i in range(len(lin_layer_sizes) - 1)])\n",
    "        \n",
    "        #for lin_layer in self.lin_layers:\n",
    "            #nn.init.kaiming_normal_(lin_layer.weight.data)\n",
    "      \n",
    "        # Output Layer\n",
    "        self.outpout_layer = nn.Linear(lin_layer_sizes[-1], outpout_size)\n",
    "        #nn.init.kaiming_normal_(self.outpout_layer.weight.data)\n",
    "    \n",
    "    \n",
    "        # Dropout Layers\n",
    "        self.dropout_layers = nn.ModuleList([nn.Dropout(rate) for rate,size in zip(lin_layer_dropouts,lin_layer_sizes)])\n",
    "\n",
    "    def forward(self, x):\n",
    "  \n",
    "        for lin_layer, dropout_layer in zip(self.lin_layers, self.dropout_layers):\n",
    "\n",
    "            x = lin_layer(x)\n",
    "        \n",
    "            x = self.activation(x)\n",
    "\n",
    "            x = dropout_layer(x)\n",
    "      \n",
    "        x = self.outpout_layer(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "# Définir la classe NeuralNetRegressor\n",
    "class NeuralNetRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, input_size, random_state, eta=0.001, max_epochs=100, batch=10, lin_layer_sizes = [50, 50],\n",
    "                 outpout_size = 1, lin_layer_dropouts = [0.4, 0.4], activation = 0):\n",
    "        self.input_size = input_size\n",
    "        self.random_state = random_state\n",
    "        self.eta = eta\n",
    "        self.max_epochs = max_epochs\n",
    "        self.batch = batch\n",
    "        self.lin_layer_sizes = lin_layer_sizes\n",
    "        self.outpout_size = outpout_size\n",
    "        self.lin_layer_dropouts = lin_layer_dropouts\n",
    "        self.activation = activation\n",
    "        self.model = FeedForwardNN(input_size, lin_layer_sizes,\n",
    "                 outpout_size, lin_layer_dropouts, activation)\n",
    "        self.criterion = nn.L1Loss() \n",
    "    \n",
    "    def fit(self, X, y, do_print=False):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.eta)\n",
    "        X_tensor = torch.tensor(X).clone().detach().float()\n",
    "        y_tensor = torch.tensor(y).clone().detach().float()\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch, shuffle=True)\n",
    "        self.model.train()\n",
    "        # Training loop\n",
    "        for epoch in range(self.max_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for batch_X, batch_y in dataloader:\n",
    "                optimizer.zero_grad()  # Reset gradients\n",
    "                outputs = self.model(batch_X)  # Forward pass\n",
    "                loss = self.criterion(outputs, batch_y)  # Compute loss\n",
    "                loss.backward()  # Backward pass\n",
    "                optimizer.step()  # Update parameters\n",
    "                epoch_loss += loss.item()\n",
    "            if do_print:\n",
    "                print(f\"Epoch {epoch+1}/{self.max_epochs}, Loss: {epoch_loss / len(dataloader)}\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor).flatten()\n",
    "        return outputs.numpy()\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def my_scorer(y_true, y_pred):\n",
    "    y_true = y_true.detach().numpy()\n",
    "    return np.mean(np.abs(y_pred-y_true)*100<=5)\n",
    "\n",
    "custom_scorer = make_scorer(my_scorer, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[114, 81]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[randint.rvs(32, 128) for _ in range(randint.rvs(1, 4))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005867127272781008"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loguniform(1e-4, 1e-1).rvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLayers:\n",
    "    def __init__(self,min_layers, max_layers, min_nodes, max_nodes):\n",
    "        self.min_nodes = min_nodes\n",
    "        self.max_nodes = max_nodes\n",
    "        self.min_layers = min_layers\n",
    "        self.max_layers = max_layers\n",
    "\n",
    "    def rvs(self, random_state=None):\n",
    "        if random_state is not None:\n",
    "            prev = randint.random_state\n",
    "            randint.random_state = random_state \n",
    "\n",
    "        res = [[randint.rvs(self.min_nodes, self.max_nodes) for _ in range(randint.rvs(self.min_layers, self.max_layers))]]\n",
    "\n",
    "        if random_state is not None:\n",
    "            randint.random_state = prev\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDropout:\n",
    "    def __init__(self, max_layers,min_dropout,max_dropout):\n",
    "        self.min_dropout = min_dropout\n",
    "        self.max_dropout = max_dropout\n",
    "        self.max_layers = max_layers\n",
    "\n",
    "    def rvs(self, random_state=None):\n",
    "        if random_state is not None:\n",
    "            prev = uniform.random_state\n",
    "            uniform.random_state = random_state \n",
    "        res = [[uniform.rvs(self.min_dropout, self.max_dropout) for _ in range(self.max_layers)]]\n",
    "\n",
    "        if random_state is not None:\n",
    "            uniform.random_state = prev\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X).clone().detach().float()\n",
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(y).clone().detach().float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MSE: -0.018972 using {'activation': 0, 'batch': 47, 'eta': 0.0017685051085424622, 'lin_layer_dropouts': [[0.6530056733559008, 0.6205716354509077, 0.4037700398666926, 0.8737659376448997]], 'lin_layer_sizes': [[105]], 'max_epochs': 335}\n"
     ]
    }
   ],
   "source": [
    "# Initialiser le modèle\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "input_size = X_train_tensor.shape[1]\n",
    "net = NeuralNetRegressor(input_size=input_size, random_state=seed)\n",
    "\n",
    "max_layers = 4\n",
    "\n",
    "# Définir les paramètres pour GridSearch\n",
    "params_dist = {\n",
    "    'eta': loguniform(1e-4, 1e-1),\n",
    "    'max_epochs': randint(200, 500),\n",
    "    'batch': randint(32, 50),\n",
    "    'lin_layer_sizes': RandomLayers(1,max_layers,32,128),  # Taille de 1 à 4 couches, entre 32 et 128 neurones par couche\n",
    "    'lin_layer_dropouts': RandomDropout(max_layers,0.,1.),  # Dropout entre 0 et 1. pour chaque couche\n",
    "    'activation': randint(0, 4),\n",
    "}\n",
    "# faire une classe qui genere les valeurs aleatoirement, et creer objet de la classe dans param dist pour lin lyer sizes et dropout faire un seule valeur pour toute les couches\n",
    "# Initialiser RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(net, params_dist, refit=True, cv=5, random_state=seed, scoring= 'neg_mean_absolute_error', verbose=0, n_iter= 5) \n",
    "\n",
    "# Entraîner le modèle avec GridSearch\n",
    "random_grid_result = random_search.fit(X_train_tensor, y_train_tensor)\n",
    "nouveau_model = random_grid_result.best_estimator_\n",
    "\n",
    "print(\"Best MSE: %f using %s\" % (random_grid_result.best_score_, random_grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred contains NaN: False\n",
      "MSE : 8.301041399378008\n",
      "t_score test : 0.9461538461538461\n",
      "t_score train : 0.9038461538461539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/93/xgf_b79s5sd1vjn_794f14ym0000gn/T/ipykernel_48663/3560239249.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nids = np.arange(1, len(predictions) + 1)\\n\\n# Create a DataFrame for the output\\noutput_df = pd.DataFrame({\\n    'ID': ids,\\n    'PURITY': predictions\\n})\\n\\n# Save the DataFrame to a CSV file\\noutput_df.to_csv('predictions.csv', index=False) \\n\\n\\n# Afficher la courbe d'apprentissage\\nimport matplotlib.pyplot as plt\\nplt.plot(steps, learning_curve)\\nplt.xlabel('Steps')\\nplt.ylabel('Loss')\\nplt.title('Learning Curve')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = nouveau_model.predict(X_valid_tensor)\n",
    "y_pred_train = nouveau_model.predict(X_train_tensor)\n",
    "predictions = nouveau_model.predict(X_test_tensor)*100\n",
    "\n",
    "# Vérifiez les sorties du modèle\n",
    "print(\"y_pred contains NaN:\", np.isnan(y_pred).any())\n",
    "\n",
    "\n",
    "# Calculer la MSE\n",
    "mse = np.mean(((y_pred - y_valid)*100)**2)\n",
    "print(\"MSE :\", mse)\n",
    "\n",
    "\n",
    "#loss = np.mean(np.maximum(0,(y_pred - y_valid)-.05) + np.maximum(0,(y_pred - y_valid)-.05))\n",
    "#print(\"Loss :\", loss)\n",
    "\n",
    "# Calculer le t_score\n",
    "train_score = np.mean(np.abs(y_pred_train-y_train)*100<=5)\n",
    "test_score = np.mean(np.abs(y_pred-y_valid)*100<=5)\n",
    "print(\"t_score test :\", train_score)\n",
    "print(\"t_score train :\", test_score)\n",
    "\n",
    "\"\"\"\n",
    "ids = np.arange(1, len(predictions) + 1)\n",
    "\n",
    "# Create a DataFrame for the output\n",
    "output_df = pd.DataFrame({\n",
    "    'ID': ids,\n",
    "    'PURITY': predictions\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_df.to_csv('predictions.csv', index=False) \n",
    "\n",
    "\n",
    "# Afficher la courbe d'apprentissage\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(steps, learning_curve)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1300, 1040]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LearningCurveDisplay, learning_curve\n\u001b[0;32m----> 2\u001b[0m train_sizes, train_scores, test_scores \u001b[38;5;241m=\u001b[39m \u001b[43mlearning_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnouveau_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_scorer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m display \u001b[38;5;241m=\u001b[39m LearningCurveDisplay(train_sizes\u001b[38;5;241m=\u001b[39mtrain_sizes,\n\u001b[1;32m      4\u001b[0m     train_scores\u001b[38;5;241m=\u001b[39mtrain_scores, test_scores\u001b[38;5;241m=\u001b[39mtest_scores, score_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m display\u001b[38;5;241m.\u001b[39mplot()\n",
      "File \u001b[0;32m~/anaconda3/envs/MLCourse/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/MLCourse/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:1906\u001b[0m, in \u001b[0;36mlearning_curve\u001b[0;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state, error_score, return_times, fit_params)\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exploit_incremental_learning \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1902\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1903\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn estimator must support the partial_fit interface \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1904\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto exploit incremental learning\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1905\u001b[0m     )\n\u001b[0;32m-> 1906\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1908\u001b[0m cv \u001b[38;5;241m=\u001b[39m check_cv(cv, y, classifier\u001b[38;5;241m=\u001b[39mis_classifier(estimator))\n\u001b[1;32m   1909\u001b[0m \u001b[38;5;66;03m# Store it as list as we will be iterating over the list multiple times\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLCourse/lib/python3.10/site-packages/sklearn/utils/validation.py:514\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[0;32m--> 514\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/MLCourse/lib/python3.10/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1300, 1040]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LearningCurveDisplay, learning_curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(nouveau_model, X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 5), cv=5, scoring=custom_scorer)\n",
    "display = LearningCurveDisplay(train_sizes=train_sizes,\n",
    "    train_scores=train_scores, test_scores=test_scores, score_name=\"Score\")\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nouveau_model.model, \"model3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1040x16 and 141x50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m toto\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Prédiction\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtoto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions)\n",
      "File \u001b[0;32m~/anaconda3/envs/MLCourse/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLCourse/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[51], line 39\u001b[0m, in \u001b[0;36mFeedForwardNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lin_layer, dropout_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_layers):\n\u001b[0;32m---> 39\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlin_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[1;32m     43\u001b[0m         x \u001b[38;5;241m=\u001b[39m dropout_layer(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/MLCourse/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLCourse/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLCourse/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1040x16 and 141x50)"
     ]
    }
   ],
   "source": [
    "toto = torch.load(\"model1.pth\")\n",
    "toto.eval()\n",
    "\n",
    "# Prédiction\n",
    "predictions = toto(X_train_tensor).flatten().detach().numpy()\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
